{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f4c5e1-4663-4519-a5ad-ca97c7908320",
   "metadata": {},
   "source": [
    "# P-Cluster Library and Job Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34d3e9-eca0-4520-b02a-5a7c2eed0e25",
   "metadata": {},
   "source": [
    "This tutorial introduces the [Spack](https://spack.io) package manager and the [SLURM](https://slurm.schedmd.com) job manager tools available on the P-Cluster.\n",
    "\n",
    "## The Spack Package Manager\n",
    "\n",
    "*Spack is a flexible package manager designed for building and managing multiple software versions in high-performance computing environments. It allows users to easily install software with different configurations, dependencies, and compilers without interference between installations. Spack supports reproducibility and portability, making it ideal for complex scientific workflows across different systems.* - ChatGPT\n",
    "\n",
    "### Setting up your environment for Spack\n",
    "The [example .bashrc](./example.bashrc) initializes Spack profile. One can use Spack to install software and generate new modules even as a non-root user, although the collection of modules on the P-Cluster is already extensive.  \n",
    "\n",
    "### Adding software to your environment using the `module` Command\n",
    "\n",
    "The `module` command is used on the P-Cluster to manage environment modules. Modules allow users to easily load, unload, and switch between different software environments without manually modifying environment variables like `PATH` and `LD_LIBRARY_PATH` manually. This command is especially useful for managing multiple versions of software or libraries in shared environments.\n",
    "\n",
    "When you load a module, it configures your environment to use a specific version of software. You can list available modules, load and unload modules, and reset your environment using various `module` subcommands.\n",
    "\n",
    "Below is a list of common `module` commands and their functions:\n",
    "\n",
    "| Command             | Description                                                                            | Example                    |\n",
    "|---------------------|----------------------------------------------------------------------------------------|----------------------------|\n",
    "| `module avail`      | Lists all available modules that can be loaded.                                         | `module avail`             |\n",
    "| `module list`       | Shows a list of currently loaded modules in your environment.                           | `module list`              |\n",
    "| `module load`       | Loads a specific module into your environment, making the software available for use.   | `module load gcc/9.3.0`    |\n",
    "| `module unload`     | Unloads a specific module, removing it from your environment.                           | `module unload gcc/9.3.0`  |\n",
    "| `module purge`      | Unloads all currently loaded modules, resetting your environment.                       | `module purge`             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce6be89",
   "metadata": {},
   "source": [
    "## The SLURM Batch System\n",
    "\n",
    "*SLURM (Simple Linux Utility for Resource Management) is an open-source batch scheduling system widely used in high-performance computing (HPC) environments to manage and allocate computational resources. It enables users to submit, schedule, and manage jobs on clusters, ensuring efficient use of available nodes and resources. SLURM provides flexible scheduling policies, supports parallel and distributed workloads, and includes features for job prioritization and resource accounting.* - ChatGPT\n",
    "\n",
    "### Common SLURM commands\n",
    "\n",
    "| Command   | Description                                                                 | Example                                |\n",
    "|-----------|-----------------------------------------------------------------------------|----------------------------------------|\n",
    "| `sbatch`  | Submits a job script to the SLURM scheduler.                                | `sbatch job_script.sh`                 |\n",
    "| `scancel` | Cancels a pending or running job.                                           | `scancel <job_id>`                     |\n",
    "| `squeue`  | Displays information about jobs in the queue.                              | `squeue`                                |\n",
    "| `sinfo`   | Displays information about available SLURM nodes and partitions.            | `sinfo`                                |\n",
    "| `salloc`  | Allocates resources for a job interactively.                               | `salloc --ntasks=2 --ntasks-per-node=2 --partition=sealevel-c5xl-demand --time=01:00:00`    |\n",
    "| `srun`    | Submits a job or launches parallel tasks (can be used in a script or interactively). | `srun --ntasks=4 ./my_program`|\n",
    "\n",
    "There are numerous resources on the web about how to use SLURM. One useful example can be found [here](https://hpc.nmsu.edu/discovery/slurm/commands/#_slurm_script_main_parts).\n",
    "\n",
    "Note that the head node (the machine where you first log in to 34.210.1.198) has very limited resources and is suitable for editing, but not for intensive data analysis or processing. We recommend using *salloc* to request an interactive node for heavy data processing. \n",
    "\n",
    "### Partition\n",
    "\n",
    "On SLURM systems, a \"partition\" is a set of compute nodes grouped for specific job submissions. Partitions define a collection of resources with particular attributes or policies where jobs are submitted, such as job limits or access to specific hardware resources. The equivalent to \"partition\" on [PBS (Portable Batch System)](https://altair.com/pbs-professional) the \"queue.\" \n",
    "\n",
    "The command `sinfo` (see the table above) can display available `SLURM` partitions. Below is the output of `sinfo` showing the available partitions on the P-Cluster.\n",
    "\n",
    "| PARTITION                  | AVAIL | TIMELIMIT | NODES | STATE | NODELIST                                           |\n",
    "|----------------------------|-------|-----------|-------|-------|----------------------------------------------------|\n",
    "| sealevel-c5n18xl-spot*      | up    | infinite  | 50    | idle~ | sealevel-c5n18xl-spot-dy-c5n18xlarge-[1-50]        |\n",
    "| sealevel-c5n18xl-demand     | up    | infinite  | 49    | idle~ | sealevel-c5n18xl-demand-dy-c5n18xlarge-[2-50]      |\n",
    "| sealevel-c5n18xl-demand     | up    | infinite  | 1     | alloc | sealevel-c5n18xl-demand-dy-c5n18xlarge-1           |\n",
    "| sealevel-c5xl-spot          | up    | infinite  | 1000  | idle~ | sealevel-c5xl-spot-dy-c5xlarge-[1-1000]            |\n",
    "| sealevel-c5xl-demand        | up    | infinite  | 1000  | idle~ | sealevel-c5xl-demand-dy-c5xlarge-[1-1000]          |\n",
    "\n",
    "There are four kinds of partitions: two are AWS c5n18xl instances, and the other two are c5xl instances. The former are suitable for large jobs, while the latter are for smaller jobs, such as interactive jobs (see [Amazon EC2 C5n instances](https://aws.amazon.com/ec2/instance-types/c5/) for more details). Partitions ending with `spot` are AWS `spot` instances, which are priced lower than `demand` instances. However, AWS can terminate a `spot` instance if there is a demand for it, meaning your job will be killed. To avoid your job being terminated by AWS, use `demand` partitions unless it is acceptable for your job to be interrupted.\n",
    "\n",
    "### Starting an interactive node\n",
    "\n",
    "As stated above, the head node (the machine where you first log in to 34.210.1.198) has very limited resources and is suitable for editing, but not for intensive data analysis or processing. We recommend using *salloc* to request an interactive node for heavy data processing. \n",
    "\n",
    "For instance, to start an interactive with machine type [c5.xlarge](https://instances.vantage.sh/aws/ec2/c5.xlarge\n",
    "), issue the following command from your home directory:\n",
    "```\n",
    "salloc --ntasks=2 --ntasks-per-node=2 --partition=sealevel-c5xl-demand --time=01:00:00 \n",
    "```\n",
    "This command requests an interactive node on the `partition` called ```sealevel-c5xl-demand``` with two tasks (a term similar to processes) running for one hour. \n",
    "\n",
    "After issuing the `salloc` command, and waiting a few minutes, one would receive the following message on the screen with a job identification number (ID), as shown below (using 123 as an example ID):\n",
    "```\n",
    "salloc: Granted job allocation 128\n",
    "salloc: Waiting for resource configuration\n",
    "```\n",
    "**`SLURM` may take several minutes to allocate and configure the requested resources.** Once the resources are ready, the prompt will appear as follows:\n",
    "```\n",
    "salloc: Nodes sealevel-c5xl-demand-dy-c5xlarge-1 are ready for job\n",
    "USERNAME@ip-10-20-22-69:~$ \n",
    "```\n",
    "Then, one can run command or executable scripts. If the interactive node is no longer needed, use `scancel JOB_ID` to exit the partition and return the requested resources.\n",
    "\n",
    "In addition to `salloc`, `srun` can also be used to request an interactive node, though it is often used to run a specific script or job. `salloc`, on the other hand, allows users to run multiple commands once the resources are allocated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d97114-f4eb-40e4-9def-e405c0bfd9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
