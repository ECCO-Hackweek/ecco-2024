{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f4c5e1-4663-4519-a5ad-ca97c7908320",
   "metadata": {},
   "source": [
    "# Getting Started with the P-Cluster \n",
    "```{attention}\n",
    "Please make sure to find some time to go through the below material before the hackweek.\n",
    "\n",
    "__Upon completion you__:\n",
    "<div>\n",
    "  <input type=\"checkbox\" name=\"a1\">\n",
    "  <label for=\"a1\">Can log in to the P-Cluster</label>\n",
    "</div>\n",
    "<div>\n",
    "  <input type=\"checkbox\" name=\"a2\">\n",
    "  <label for=\"a2\">Update the .bashrc file in your home directory on the P-Cluster</label>\n",
    "</div>\n",
    "```\n",
    "The P-Cluster is very much like a high-performance computing (HPC) system where you can run interactive jobs and/or perform parallel computing. Our [P-Cluster](https://docs.aws.amazon.com/parallelcluster/latest/ug/what-is-aws-parallelcluster.html) uses compute resources from the AWS cloud.\n",
    "## Connecting to the P-Cluster\n",
    "### Generating and Submitting Your SSH Key\n",
    "\n",
    "Before you can connect to the P-Cluster, please send your desired username and a public SSH key to [eccohackweek@gmail.com](mailto:eccohackweek@gmail.com) so you can be added to the authorized user list. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2a9147-40ae-454f-b8be-c45c3baf6314",
   "metadata": {},
   "source": [
    "You can generate a public/private key pair on your local machine using a tool like ssh-keygen:\n",
    "\n",
    "```\n",
    "ssh-keygen -t rsa -b 4096 -C KEY_LABEL\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8f9390-df6f-48f9-992e-a55c7c1d7cc4",
   "metadata": {},
   "source": [
    "After running the command, ssh-keygen will prompt you with a series of questions, including where to save the public/private key pair and whether to use a passphrase. It will suggest saving the keys in ```~/.ssh/```, with filenames ```id_rsa``` (private key) and ```id_rsa.pub``` (public key) but you can specify a different location and filenames. You will also be prompted to enter a passphrase, which is an option to add an extra layer of security (you will need to enter it each time you use the key). If you prefer not to use a passphrase, simply press *Enter* twice when prompted by ssh-keygen.\n",
    "\n",
    "### Connecting to the P-Cluster via SSH\n",
    "From your local Linux or Mac terminal, connect to the P-Cluster using its IP address (34.210.1.198)\n",
    "\n",
    "```\n",
    "ssh -i /path/to/privatekey -X USERNAME@34.210.1.198\n",
    "```\n",
    "\n",
    "The -X option in the ssh -X command enables X11 forwarding, allowing you to run graphical applications on a remote machine and display them locally on your computer.\n",
    "\n",
    "You will then be prompted to enter your passphrase:\n",
    "```\n",
    "Enter passphrase for key '/Users/USERNAME/.ssh/id_rsa':  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34d3e9-eca0-4520-b02a-5a7c2eed0e25",
   "metadata": {},
   "source": [
    "## P-Cluster Overview\n",
    "### Directory Structure\n",
    "Once you successfully log in, you will be in your home directory, /home/USERNAME/. Each user also has a directory at /efs/USERNAME/. The home directory is intended for storing analysis code, scripts, configurations, and user-specific settings, while the /efs/USERNAME/ directory is better suited for conducting model runs.\n",
    "\n",
    "### Shell Environment \n",
    "By default, the P-Cluster uses Bash as the shell environment. Each user has a .bashrc file in their home directory, which is automatically executed whenever a new Bash session starts. This file can be customized to set environment variables, create aliases, and configure other shell settings. Here is an [example .bashrc](./example.bashrc) that sets the default Bash environment on the P-Cluster. This [example .bashrc](./example.bashrc) can serve as a starting point for further customization (download it, place it in your home directory, and rename it to .bashrc).\n",
    "\n",
    "If preferred, users can switch to other shells such as Csh, Ksh, and Zsh. \n",
    "\n",
    "### Spack\n",
    "The P-Cluster uses Spack (https://spack.io), an open-source package manager, to manage modules and libraries. The [example .bashrc](./example.bashrc) initializes Spack profile. One can use Spack to install software and generate new modules even as a non-root user, although the collection of modules on the P-Cluster is already extensive.  \n",
    "\n",
    "### The `module` Command\n",
    "\n",
    "The `module` command is used on the P-Cluster to manage environment modules. Modules allow users to easily load, unload, and switch between different software environments without manually modifying environment variables like `PATH` and `LD_LIBRARY_PATH` manually. This command is especially useful for managing multiple versions of software or libraries in shared environments.\n",
    "\n",
    "When you load a module, it configures your environment to use a specific version of software. You can list available modules, load and unload modules, and reset your environment using various `module` subcommands.\n",
    "\n",
    "Below is a list of common `module` commands and their functions:\n",
    "\n",
    "| Command             | Description                                                                            | Example                    |\n",
    "|---------------------|----------------------------------------------------------------------------------------|----------------------------|\n",
    "| `module avail`      | Lists all available modules that can be loaded.                                         | `module avail`             |\n",
    "| `module list`       | Shows a list of currently loaded modules in your environment.                           | `module list`              |\n",
    "| `module load`       | Loads a specific module into your environment, making the software available for use.   | `module load gcc/9.3.0`    |\n",
    "| `module unload`     | Unloads a specific module, removing it from your environment.                           | `module unload gcc/9.3.0`  |\n",
    "| `module purge`      | Unloads all currently loaded modules, resetting your environment.                       | `module purge`             |\n",
    "\n",
    "### Batch System\n",
    "The P-Cluster uses Slurm (https://slurm.schedmd.com) as the batch system for submitting jobs for MPI runs. Some commonly used commands are listed below.\n",
    "\n",
    "| Command   | Description                                                                 | Example                                |\n",
    "|-----------|-----------------------------------------------------------------------------|----------------------------------------|\n",
    "| `sbatch`  | Submits a job script to the Slurm scheduler.                                | `sbatch job_script.sh`                 |\n",
    "| `scancel` | Cancels a pending or running job.                                           | `scancel <job_id>`                     |\n",
    "| `squeue`  | Displays information about jobs in the queue.                              | `squeue`                                |\n",
    "| `sinfo`   | Displays information about available Slurm nodes and partitions.            | `sinfo`                                |\n",
    "| `salloc`  | Allocates resources for a job interactively.                               | `salloc --ntasks=2 --ntaske-per-node=2 --partition=sealevel-c5xl-demand --time=01:00:00`    |\n",
    "| `srun`    | Submits a job or launches parallel tasks (can be used in a script or interactively). | `srun --ntasks=4 ./my_program`|\n",
    "\n",
    "There are numerous resources on the web about how to use Slurm. One useful example can be found [here](https://hpc.nmsu.edu/discovery/slurm/commands/#_slurm_script_main_parts).\n",
    "\n",
    "Note that the head node (the machine where you first log in to 34.210.1.198) has very limited resources and is suitable for editing, but not for intensive data analysis or processing. We recommend using salloc to request an interactive node for heavy data processing. For instance, issue the following command:\n",
    "```\n",
    "salloc --ntasks=2 --ntasks-per-node=2 --partition=sealevel-c5xl-demand --time=01:00:00 \n",
    "```\n",
    "This command requests an interactive node on the `partition` `sealevel-c5xl-demand` with two tasks (a term similar to processes) running on it for one hour. Note that the term `partition` is analogous to `queue` in [Portable Batch System](https://altair.com/pbs-professional), a.k.a, `PBS`, another commonly used batch system that users may be familiar with. After issuing the `salloc` command, one would receive the following message on the screen with a job identification number (ID), as shown below (using 123 as an example ID):\n",
    "```\n",
    "salloc: Granted job allocation 123\n",
    "```\n",
    "**`Slurm` may take several minutes to allocate and configure the requested resources.** Once the resources are ready, the prompt will appear as follows:\n",
    "```\n",
    "USERANME@ip-10-20-22-69:/efs/USERNAME$ \n",
    "```\n",
    "Then, one can run command or executable scripts. If the interactive node is no longer needed, use `scancel JOB_ID` to exit the partition and return the requested resources.\n",
    "\n",
    "In addition to `salloc`, `srun` can also be used to request an interactive node, though it is often used to run a specific script or job. `salloc`, on the other hand, allows users to run multiple commands once the resources are allocated."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
