{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing ECCO Version 4 Release 4\n",
    "ECCO Version 4 Release 4 ([V4r4](https://ecco-group.org/products-ECCO-V4r4.htm)) is ECCO's latest publicly available central estimate (see its data repository on [PO.DAAC](https://podaac.jpl.nasa.gov/ECCO?tab=mission-objectives&sections=about%2Bdata), which has been used in numerous studies (e.g., [Wu et al. (2020)](https://www.science.org/doi/10.1126/science.abb9519)). [Wang et al. (2023)](https://doi.org/10.5281/zenodo.10038354) provides detailed instructions on how to reproduce the ECCO V4r4 estimate. In this tutorial, we follow the instructions and reproduce the ECCO V4r4 estimate on the P-Cluster. \n",
    "\n",
    "## Obtaining Code and Run-time Parameters\n",
    "\n",
    "We first connect to the P-Cluster and change the directory to the user's directory on /efs_ecco, as described in the [P-Cluster introduction tutorial](../../preliminary/pcluster-login.ipynb):\n",
    "```\n",
    "ssh -i /path/to/privatekey -X USERNAME@34.210.1.198\n",
    "cd /efs_ecco/USERNAME\n",
    "```\n",
    "\n",
    "We then follow Sections 2 and 3 of [Wang et al. (2023)](https://doi.org/10.5281/zenodo.10038354) to download the MITgcm checkpoint (c66g), release-specific code, and run-time parameters. The input files, such as atmospheric forcing and initial conditions, as described in Section 4 of [Wang et al. (2023)](https://doi.org/10.5281/zenodo.10038354), are several hundreds gigabytes in size. For the sake of time,these input files have been downloaded and stored on the P-Cluster. These files will be linked in the example run script described below. \n",
    "\n",
    "## Modules \n",
    "The modules used in the P-Cluster differ from those specified in Section 5 of [Wang et al. (2023)](https://doi.org/10.5281/zenodo.10038354). They have been loaded in the [example .bashrc](example.bashrc), which should have been downloaded and renamed to /home/USERNAME/.bashrc as described in the [P-Cluster introduction tutorial](../../preliminary/pcluster-login.ipynb), so that the required modules are loaded automatically. Specificity, the modules are loaded in [example .bashrc](example.bashrc) as follows:\n",
    "\n",
    "| Module Load Command                                                       |\n",
    "|---------------------------------------------------------------------------|\n",
    "| module load intel-oneapi-compilers-2021.2.0-gcc-11.1.0-adt4bgf            |\n",
    "| module load intel-oneapi-mpi-2021.2.0-gcc-11.1.0-ibxno3u                  |\n",
    "| module load netcdf-c-4.8.1-gcc-11.1.0-6so76nc                             |\n",
    "| module load netcdf-fortran-4.5.3-gcc-11.1.0-d35hzyr                       |\n",
    "| module load hdf5-1.10.7-gcc-9.4.0-vif4ht3                                 |\n",
    "\n",
    "With these modules pre-loaded by .bashrc, one can skip the module-loading step in the first box of Section 5.1 of [Wang et al. (2023)](https://doi.org/10.5281/zenodo.10038354) and proceed directly to the compilation steps in the second box of Section 5.1 of  [Wang et al. (2023)](https://doi.org/10.5281/zenodo.10038354). \n",
    "\n",
    "## Compile\n",
    "\n",
    "The steps for compiling the code the same as described in the second box of Section 5.1 of  [Wang et al. (2023)](https://doi.org/10.5281/zenodo.10038354) except for one important change: you need to specify the `optfile` as `../code/linux_ifort_impi_aws_sysmodule`.\n",
    "\n",
    "```\n",
    "cd WORKINGDIR/ECCOV4/release4\n",
    "mkdir build\n",
    "cd build\n",
    "export ROOTDIR=../../../MITgcm\n",
    "../../../MITgcm/tools/genmake2 -mods=../code -optfile=../code/linux_amd64_ifort+mpi_ice_nas -mpi\n",
    "make -j16 depend\n",
    "make -j16 all\n",
    "cd ..\n",
    "```\n",
    "This `optfile` has been specifically customized for the P-Cluster. If successful, the executable `mitgcmuv` will be generated in the `build` directory.\n",
    "\n",
    "## Run the Model\n",
    "After one has issued the compilation steps, successfully compiled the code, and generated the executable in the `build` directory (`WORKINGDIR/ECCOV4/release4/build/mitgcmuv`), one can proceed with running the model. For this purpose, we provide an [example run script](run_script_slurm.bash) that will integrate the model for three months (See below for how to change the [run script](run_script_slurm.bash) to conduct a run over the period of 1992-2017, the entire V4r4 integration period). \n",
    "\n",
    "### SLURM Directives\n",
    "As described in the [P-Cluster introduction tutorial](../../preliminary/pcluster-login.ipynb), the P-Cluster uses SLURM as the batch system. There are a few SLURM directives at the beginning of the [run script](run_script_slurm.bash) that request the necessary resources for conducting the run. These SLURM directives are as follows:\n",
    "| SBATCH Commands                                | Description                                                                                                            |\n",
    "|------------------------------------------------|------------------------------------------------------------------------------------------------------------------------|\n",
    "| #SBATCH -J ECCOv4r4                            | Job name is ECCOv4r4.                                                                                                  |\n",
    "| #SBATCH --nodes=3                              | Request three nodes.                                                                                                   |\n",
    "| #SBATCH --ntasks-per-node=36                   | Each node has 36 tasks (processes).                                                                                    |\n",
    "| #SBATCH --time=24:00:00                        | Request a wall clock time of 24 hours.                                                                                 |\n",
    "| #SBATCH --exclusive                            | No other jobs will be scheduled on the same nodes while the job is running.                                             |\n",
    "| #SBATCH --partition=sealevel-c5n18xl-demand    | Request an [on-demand](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html) [Amazon EC2 C5n instances](https://aws.amazon.com/ec2/instance-types/c5/) (Tab C5n under Product Details) that has 72 vCPU and 192 GB memory. |\n",
    "| #SBATCH --mem-per-cpu=1GB                      | Each CPU/process/task has 1GB memory.                                                                                  |\n",
    "| #SBATCH -o ECCOv4r4-%j-out                     | Batch output log file.                                                                                                 |\n",
    "| #SBATCH -e ECCOv4r4-%j-out                     | Batch error log file.                                                                                                  |\n",
    "\n",
    "### Submit the Run and Check Job Status\n",
    "To submit the run, issue the following command at /efs_ecco/USERNAME/ECCO/V4/r4/WORKINGDIR/ECCOV4/release4\n",
    "```\n",
    "sbatch run_script_slurm.bash\n",
    "```\n",
    "Once submitting the job, SLURM will generate a job id and show the following message:\n",
    "```\n",
    "Submitted batch job 123\n",
    "```\n",
    "\n",
    "We can then check the status the job by using the following command:\n",
    "```\n",
    "squeue\n",
    "```\n",
    "Usually, SLURM takes several minutes to configure a job, with the status (`ST`) showing `CF` (for configuring): \n",
    "```\n",
    "             JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)\n",
    "               123 sealevel- ECCOv4r4 USERNAME  CF       0:53      3 sealevel-c5n18xl-demand-dy-c5n18xlarge-[1-3]\n",
    "```\n",
    "After a while, squeue will show the status changing to `R` (for run) as shown in following:\n",
    "\n",
    "```\n",
    "             JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)\n",
    "               123 sealevel- ECCOv4r4 USERNAME  R        3:30      3 sealevel-c5n18xl-demand-dy-c5n18xlarge-[1-3]\n",
    "```\n",
    "The run directory is /efs_ecco/USERNAME/ECCO/V4/r4/WORKINGDIR/ECCOV4/release4/run. The 3-month integration takes less than 20 minutes to complete. `NORMAL END` inside the Batch log file `/efs_ecco/USERNAME/ECCO/V4/r4/WORKINGDIR/ECCOV4/release4/ECCOv4r4-123-out` indicates a successfully completed run. The run will output monthly means and snapshots of diagnostic fields in `/efs_ecco/USERNAME/ECCO/V4/r4/WORKINGDIR/ECCOV4/release4/run/diags/`. These fields can be analyzed using Jupyter Notebooks presented in some of the ECCO Hackathon tutorials. \n",
    "\n",
    "To conduct the entire 26-year (1992–2017) run, comment out the following three lines in the [script](run_script_slurm.bash): \n",
    "```\n",
    "unlink data\n",
    "cp -p ../namelist/data .\n",
    "sed -i '/#nTimeSteps=2160,/ s/^#//; /nTimeSteps=227903,/ s/^/#/' data\n",
    "```\n",
    "\n",
    "## References\n",
    "Wang, O., & Fenty, I. (2023). Instructions for reproducing ECCO Version 4 Release 4 (1.5). Zenodo. https://doi.org/10.5281/zenodo.10038354\n",
    "\n",
    "Wu, W., Zhan, Z., Peng, S., Ni, S., & Callies, J. (2020). Seismic ocean thermometry. Science, 1515(6510), 1510–1515. https://doi.org/10.1126/science.abb9519"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
